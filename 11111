import pandas as pd
import numpy as np
import xgboost as xgb
import shap
import optuna
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder

# ================== 1. 参数定义 ===================
# 替换为你的变量
target_col = 'target'
categorical_cols = [...]  # 比如 ['state', 'claim_type']
feature_cols = [...]      # 所有用于建模的特征列（包括分类变量）

# ================== 2. 数据预处理 ===================
df_encoded = df.copy()

# 编码分类变量
encoder = OrdinalEncoder()
df_encoded[categorical_cols] = encoder.fit_transform(df_encoded[categorical_cols])

# 拆分训练/验证
X = df_encoded[feature_cols]
y = df_encoded[target_col]
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# ================== 3. Optuna 超参数调优 ===================
def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),
        'alpha': trial.suggest_float('alpha', 1e-3, 10.0),
    }

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    booster = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    preds = booster.predict(dvalid)
    rmse = mean_squared_error(y_valid, preds, squared=False)
    return rmse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
best_params = study.best_trial.params
best_params.update({
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'tree_method': 'hist'
})

# ================== 4. 训练最终模型 ===================
dtrain_full = xgb.DMatrix(X, label=y)
final_model = xgb.train(best_params, dtrain_full, num_boost_round=study.best_trial.number)

# ================== 5. SHAP 分析 ===================
explainer = shap.Explainer(final_model, X)
shap_values = explainer(X)

# SHAP 图
shap.summary_plot(shap_values, X)

# ================== 6. 输出预测（可选） ===================
df_encoded['predicted'] = final_model.predict(xgb.DMatrix(X))

# 保存结果（可选）
# df_encoded.to_csv("train_with_predictions.csv", index=False)
