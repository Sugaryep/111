import pandas as pd
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# 假设你的数据是 df，目标列名是 'target'，分类列名列表是 categorical_cols
# 1. 转换分类变量为 pandas 的 Categorical 类型
for col in categorical_cols:
    df[col] = df[col].astype('category')

# 2. 拆分训练集和验证集
X = df.drop(columns=['target'])
y = df['target']
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. 定义 Optuna 目标函数
def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'enable_categorical': True,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),
        'alpha': trial.suggest_float('alpha', 1e-3, 10.0),
    }

    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
    dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)

    booster = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    preds = booster.predict(dvalid)
    rmse = mean_squared_error(y_valid, preds, squared=False)  # 取 RMSE
    return rmse

# 4. 运行 Optuna
study = optuna.create_study(direction='minimize')  # 回归任务最小化误差
study.optimize(objective, n_trials=50)

print("Best trial:")
print(study.best_trial.params)

# 5. 使用最佳参数训练最终模型
best_params = study.best_trial.params
best_params.update({
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'tree_method': 'hist',
    'enable_categorical': True
})

# 用全部数据训练最终模型
dtrain_full = xgb.DMatrix(X, label=y, enable_categorical=True)
final_model = xgb.train(best_params, dtrain_full, num_boost_round=study.best_trial.number)

# 保存模型
final_model.save_model("xgb_regression_optuna_model.json")


# 训练集预测
dtrain = xgb.DMatrix(X_train, enable_categorical=True)
train_preds = final_model.predict(dtrain)

# 验证集预测
dvalid = xgb.DMatrix(X_valid, enable_categorical=True)
valid_preds = final_model.predict(dvalid)

# 恢复训练/验证集原始结构，加上预测值
df_train_result = X_train.copy()
df_train_result['target'] = y_train.values
df_train_result['matternumber'] = id_train.values
df_train_result['predicted'] = train_preds

df_valid_result = X_valid.copy()
df_valid_result['target'] = y_valid.values
df_valid_result['matternumber'] = id_valid.values
df_valid_result['predicted'] = valid_preds

# 如果你想把 predicted 移到最后一列（可选）
def move_column_to_end(df, col_name):
    cols = [c for c in df.columns if c != col_name] + [col_name]
    return df[cols]

df_train_result = move_column_to_end(df_train_result, 'predicted')
df_valid_result = move_column_to_end(df_valid_result, 'predicted')

# 可选：查看结果
print("训练集预测结果：")
print(df_train_result.head())

print("验证集预测结果：")
print(df_valid_result.head())













import pandas as pd
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 所有特征列（必须只包含用于建模的特征列）
# 比如：feature_cols = ['age', 'claim_type', 'state', 'some_numeric']
# categorical_cols 应该是 feature_cols 的子集
# target 列是你要预测的目标变量

# 1. 转换分类变量
for col in categorical_cols:
    df[col] = df[col].astype('category')

# 2. 明确特征和标签（保留 DataFrame 其他部分不动）
X = df[feature_cols]
y = df['target']

# 3. 拆分训练/验证集（保持原 DataFrame 的结构）
df_train, df_valid = train_test_split(df, test_size=0.2, random_state=42)

# 从中提取对应的 X, y
X_train = df_train[feature_cols]
y_train = df_train['target']

X_valid = df_valid[feature_cols]
y_valid = df_valid['target']

# 4. Optuna 目标函数
def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'enable_categorical': True,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),
        'alpha': trial.suggest_float('alpha', 1e-3, 10.0),
    }

    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
    dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)

    booster = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    preds = booster.predict(dvalid)
    rmse = mean_squared_error(y_valid, preds, squared=False)
    return rmse

# 5. 调参
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

# 6. 用最佳参数训练模型
best_params = study.best_trial.params
best_params.update({
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'tree_method': 'hist',
    'enable_categorical': True
})

dtrain_full = xgb.DMatrix(X[feature_cols], label=y, enable_categorical=True)
final_model = xgb.train(best_params, dtrain_full, num_boost_round=study.best_trial.number)

# 7. 对训练集和验证集分别预测，并加上 predicted 列（不打乱其他列）
dtrain_pred = xgb.DMatrix(X_train, enable_categorical=True)
df_train['predicted'] = final_model.predict(dtrain_pred)

dvalid_pred = xgb.DMatrix(X_valid, enable_categorical=True)
df_valid['predicted'] = final_model.predict(dvalid_pred)

# 可选：拼接训练 + 验证集作为总结果
df_full_result = pd.concat([df_train, df_valid]).sort_index()

# 可选保存
# df_full_result.to_csv("train_valid_with_predictions.csv", index=False)
